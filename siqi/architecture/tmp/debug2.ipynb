{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import string\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import importlib\n",
    "import time\n",
    "import cPickle \n",
    "import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import lasagne\n",
    "import collections\n",
    "import getopt\n",
    "\n",
    "import config.config_dense as CD\n",
    "import config.config_hyper as CH\n",
    "import config.contact_util as CU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TOL = 1e-5\n",
    "start_epoch = 0\n",
    "num_epochs = 36\n",
    "MAX_LEN = 400\n",
    "classes = 12\n",
    "lambda_reg = 0.0001\n",
    "cut_norm = 10\n",
    "output_dir = '/mnt/home/siqi/x_models/debug'\n",
    "config_name = 'config_dense'\n",
    "shuffle = True\n",
    "\n",
    "optimizer = 'sgd'\n",
    "init_lr = 0.01\n",
    "reduce_lr = True \n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "experiment_id = \"%s-%s-%s-%s-%s\" % (config_name, optimizer, str(init_lr), str(reduce_lr), timestamp)\n",
    "metadata_path = os.path.join( output_dir, experiment_id)\n",
    "\n",
    "response = 1 if classes == 12 else 0\n",
    "print 'output path is', metadata_path, ', response is', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Build Models ############\n",
    "l_in_1, l_in_2, l_in_3, l_1dout = CH.OneDResNet()\n",
    "l_out = CD.DenseNet(l_1dout, l_in_1, classes=classes, depth=81, first_output=5, growth_rate=12, num_blocks=8, dropout=0.0,\\\n",
    "                    filter_size = 3, n_hidden1 = 1000, n_hidden2 = 1000)\n",
    "CU.SummaryNet(l_out, num_para=True, print_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sym_y = T.itensor3('target_contact_map')\n",
    "sym_mask = T.itensor3('mask')\n",
    "sym_weight = T.tensor3('contact_weight')\n",
    "\n",
    "grad_start_time = time.time()\n",
    "out_train = lasagne.layers.get_output(l_out, deterministic=False)\n",
    "out_eval = lasagne.layers.get_output(l_out, deterministic=True, batch_norm_use_averages=False)\n",
    "pred_train = out_train[:, :, :, :].reshape((-1, classes))\n",
    "pred_valid = out_eval [:, :, :, :].reshape((-1, classes))\n",
    "\n",
    "params = lasagne.layers.get_all_params(l_out, regularizable=True)\n",
    "reg_term = sum(T.sum(p**2) for p in params)\n",
    "\n",
    "######### calculate cost function for train #######\n",
    "cost_train_cc = lasagne.objectives.categorical_crossentropy(T.clip(pred_train, TOL, 1-TOL), sym_y.flatten())\n",
    "cost_train_weight_cc = T.sum( cost_train_cc * sym_mask.flatten() * sym_weight.flatten() ) / \\\n",
    "                       T.sum( sym_weight.flatten() * sym_mask.flatten() )\n",
    "cost_train = cost_train_weight_cc + lambda_reg * reg_term\n",
    "\n",
    "######### calculate cost function for inference #######\n",
    "cost_valid_cc = lasagne.objectives.categorical_crossentropy(T.clip(pred_valid, TOL, 1-TOL), sym_y.flatten())\n",
    "cost_valid_weight_cc = T.sum(cost_valid_cc*sym_mask.flatten()*sym_weight.flatten()) / \\\n",
    "                       T.sum(sym_weight.flatten()*sym_mask.flatten())   # masked loss\n",
    "cost_inference = cost_valid_weight_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Compile Models ############\n",
    "sh_lr = theano.shared(lasagne.utils.floatX(init_lr))\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = [T.grad(cost_train, p, consider_constant= [sym_weight]) for p in all_params] \n",
    "print \"Creating cost function and computing grads...\"\n",
    "updates, norm_calc = lasagne.updates.total_norm_constraint(all_grads, max_norm=cut_norm, return_norm=True)\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    print 'Using Adam Optimizer, with init step size', sh_lr.get_value(),\n",
    "    updates = lasagne.updates.adam(updates, all_params, learning_rate=sh_lr)\n",
    "elif optimizer == 'sgd':\n",
    "    print 'Using SGD Momentom Optimizer, with step size', sh_lr.get_value(),\n",
    "    updates = lasagne.updates.nesterov_momentum(updates, all_params, sh_lr, 0.9)\n",
    "print ', updates done, time cosuming', time.time() - grad_start_time, 's'\n",
    "\n",
    "print 'compiling train and evals...'\n",
    "t_compile = time.time()\n",
    "train = theano.function([l_in_1.input_var, l_in_2.input_var, l_in_3.input_var, sym_y, sym_mask, sym_weight], \\\n",
    "                        [cost_train, cost_train_cc, cost_train_weight_cc, lambda_reg*reg_term, out_train], \\\n",
    "                        updates=updates, allow_input_downcast=True)\n",
    "\n",
    "eval_valid  = theano.function([l_in_1.input_var, l_in_2.input_var, l_in_3.input_var, sym_y, sym_mask, sym_weight], \\\n",
    "                        [cost_inference, cost_valid_cc, cost_valid_weight_cc, lambda_reg*reg_term, out_eval], \\\n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "#eval_train  = theano.function([l_in_1.input_var, l_in_2.input_var, l_in_3.input_var, sym_y, sym_mask, sym_weight], \\\n",
    "#                        [cost_train, cost_train_cc, cost_train_weight_cc, lambda_reg*reg_term, out_train], \\\n",
    "#                        allow_input_downcast=True)\n",
    "print \"compile time %fs\" %(time.time()-t_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Load Data ############\n",
    "print \"loading data ...\",\n",
    "start_time = time.time()\n",
    "feats = cPickle.load( open('/mnt/home/siqi/NewContact/TrainFeats/feats_train.pkl') )\n",
    "weight = cPickle.load( open('/mnt/home/siqi/NewContact/TrainFeats/weights_train.pkl') )\n",
    "contact = cPickle.load( open('/mnt/home/siqi/NewContact/TrainFeats/contact_train.pkl') )\n",
    "\n",
    "x_train = feats['train_feat']; x_valid = feats['valid_feat']\n",
    "w_train = weight['train_weight']; w_valid = weight['valid_weight']\n",
    "y_train = contact['train_contact']; y_valid = contact['valid_contact']\n",
    "\n",
    "feats = None; weight = None; contact = None\n",
    "print \"completed ..., it takes\", time.time() - start_time, 's'\n",
    "\n",
    "print 'Truncate data to MAX_LEN =', MAX_LEN\n",
    "CU.TruncateTrainData(x_train, y_train, w_train, MAX_LEN = MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Train ############\n",
    "log_file = open( output_dir + '/log.' + experiment_id, 'w', 0)\n",
    "\n",
    "loss_train_mean = []; acc_train_mean = []\n",
    "loss_valid_mean = []; acc_valid_mean = []\n",
    "loss_valid_mean2 = []; acc_valid_mean2 = []\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if shuffle:\n",
    "        combined = list(zip(x_train, w_train, y_train))\n",
    "        random.shuffle(combined)\n",
    "        x_train[:], w_train[:],  y_train[:] = zip(*combined)\n",
    "    ################   Train  ##########################\n",
    "    loss_train_epoch = []; weight_train_epoch = [];  acc_train_epoch = []\n",
    "    print 'epoch', epoch, 'with lr =', np.round( sh_lr.get_value(), 6)\n",
    "    sys.stdout.flush\n",
    "    \n",
    "    for i in range(4000, 4005):\n",
    "        sys.stdout.write('\\r%d/%d for train'%(i+1, len(x_train) ))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        l, w, a = CU.RunFuncs(x_train[i], y_train[i], w_train[i], train)\n",
    "        loss_train_epoch.append(l); weight_train_epoch.append(w); acc_train_epoch.append(a)       \n",
    "    loss_train_mean.append( np.average(loss_train_epoch, axis=0, weights= weight_train_epoch) )\n",
    "    acc_train_mean.append( np.mean( np.row_stack(acc_train_epoch), 0) )\n",
    "    print '\\t', ' '.join( [str(np.round(t,5)) for t in loss_train_mean[-1][[0,2,3]]] ), acc_train_mean[-1][4]\n",
    "    \n",
    "    ################   VALID  ##########################\n",
    "    loss_valid_epoch = []; weight_valid_epoch = [];  acc_valid_epoch = []\n",
    "    for i in range(380, 387):\n",
    "        sys.stdout.write('\\r%d/%d for valid'%(i+1, len(x_valid) ))\n",
    "        sys.stdout.flush()\n",
    "        l, w, a = CU.RunFuncs(x_valid[i], y_valid[i], w_valid[i], eval_valid)\n",
    "        loss_valid_epoch.append(l); weight_valid_epoch.append(w); acc_valid_epoch.append(a)       \n",
    "    loss_valid_mean.append( np.average(loss_valid_epoch, axis=0, weights= weight_valid_epoch) )\n",
    "    acc_valid_mean.append( np.mean( acc_valid_epoch, 0) )\n",
    "    print '\\t', ' '.join( [str(np.round(t,5)) for t in loss_valid_mean[-1][[0,2,3]]] ), acc_valid_mean[-1][0][4]\n",
    "\n",
    "    if reduce_lr and ( epoch+1 == (num_epochs-start_epoch) * 0.5 or \\\n",
    "                      epoch+1 == (num_epochs-start_epoch) * 0.8 ):\n",
    "        new_lr = sh_lr.get_value() * 0.1\n",
    "        sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        \n",
    "    ################   Dump Models and Log  ##########################\n",
    "\n",
    "    with open((metadata_path + \"-%d\" % (epoch) + \".pkl\"), 'w') as f:\n",
    "        cPickle.dump({'config_name': config_name, 'param_values': lasagne.layers.get_all_param_values(l_out)}, f, \\\n",
    "                 protocol=pickle.HIGHEST_PROTOCOL)\n",
    "       \n",
    "    print >> log_file, epoch+1, \n",
    "    print >> log_file, ' '.join( [str(np.round(t,5)) for t in loss_train_mean[-1][[0,2,3]]] ), \n",
    "    print >> log_file, ' '.join( [str(np.round(t,5)) for t in loss_valid_mean[-1][[0,2,3]]]), \n",
    "    print >> log_file, ' '.join(map(str, np.round(acc_train_mean[-1],4))),\n",
    "    print >> log_file, ' '.join(map(str, np.round(acc_valid_mean[-1][0],4))),\n",
    "    print >> log_file, time.time() - start_time\n",
    "log_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_valid_mean[-1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean( acc_valid_epoch, 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "acc_train_mean[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
