{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import string\n",
    "import sys\n",
    "from datetime import datetime, timedelta\n",
    "import importlib\n",
    "import time\n",
    "import cPickle \n",
    "import cPickle as pickle\n",
    "#import src.utils_contact as utils\n",
    "#from utils_contact import confusionMatrix, TopAccuracyByRange\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import lasagne\n",
    "import collections\n",
    "\n",
    "np.random.seed(1234)\n",
    "sys.setrecursionlimit(15000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ... completed ..., it takes 10.0629279613 s\n"
     ]
    }
   ],
   "source": [
    "##### Load Data ############\n",
    "print \"loading data ...\",\n",
    "start_time = time.time()\n",
    "feats = cPickle.load( open('/mnt/home/siqi/NewContact/TrainFeats/feats_train.pkl') )\n",
    "weight = cPickle.load( open('/mnt/home/siqi/NewContact/TrainFeats/weights_train.pkl') )\n",
    "contact = cPickle.load( open('/mnt/home/siqi/NewContact/TrainFeats/contact_train.pkl') )\n",
    "\n",
    "x_train = feats['train_feat']; x_valid = feats['valid_feat']\n",
    "w_train = weight['train_weight']; w_valid = weight['valid_weight']\n",
    "y_train = contact['train_contact']; y_valid = contact['valid_contact']\n",
    "\n",
    "feats = None; weight = None; contact = None\n",
    "print \"completed ..., it takes\", time.time() - start_time, 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 4758 epochs, each epoch has 5 elements, the meaning of each elements\n",
      "\t0. 1D feats (28, 27, 26)\n",
      "\t1. 2D feats (28, 27, 27, 5)\n",
      "\t2. 1D mask (28, 27)\n",
      "\t3. 2D mask (28, 27, 27)\n",
      "\t4. Other 1D feat... (28, 27, 78)\n",
      "\n",
      "for each weight, we have  (0) 3 label weight   and   (1) 12 label weight\n",
      "\n",
      "for label, we have (0) 3 label  (1) 12 label (2) C_b distance\n"
     ]
    }
   ],
   "source": [
    "##### Summary ############\n",
    "\n",
    "print 'we have', len(x_train), 'epochs, each epoch has', len( x_train[0] ), 'elements, the meaning of each elements'\n",
    "print '\\t0. 1D feats', x_train[-1][0].shape\n",
    "print '\\t1. 2D feats', x_train[-1][1].shape\n",
    "print '\\t2. 1D mask', x_train[-1][2].shape\n",
    "print '\\t3. 2D mask', x_train[-1][3].shape\n",
    "print '\\t4. Other 1D feat...', x_train[-1][4].shape\n",
    "\n",
    "print '\\nfor each weight, we have  (0) 3 label weight   and   (1) 12 label weight'\n",
    "\n",
    "print '\\nfor label, we have (0) 3 label  (1) 12 label (2) C_b distance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output path is /mnt/home/siqi/x_models/debug/retrain_config_dense-20170214-110508 , response is 1\n"
     ]
    }
   ],
   "source": [
    "##### Hyper Parameters ############\n",
    "\n",
    "TOL = 1e-5\n",
    "num_epochs = 20\n",
    "classes = 12\n",
    "lambda_reg = 0.0001\n",
    "cut_norm = 10\n",
    "optimizer = 'adam'\n",
    "init_lr = 0.01\n",
    "reduce_lr = True\n",
    "output_dir = '/mnt/home/siqi/x_models/debug'\n",
    "config_name = 'config_dense'\n",
    "shuffle = False\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "experiment_id = \"%s-%s\" % (config_name, timestamp)\n",
    "metadata_path = os.path.join( output_dir,  \"retrain_%s\" % experiment_id)\n",
    "\n",
    "response = 1 if classes == 12 else 0\n",
    "print 'output path is', metadata_path, ', response is', response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters is 433625\n"
     ]
    }
   ],
   "source": [
    "import config_dense as CD\n",
    "import config_hyper as CH\n",
    "import contact_util as CU\n",
    "\n",
    "l_in_1, l_in_2, l_in_3, l_1dout = CH.OneDResNet()\n",
    "\n",
    "l_out = CD.DenseNet(l_1dout, l_in_1, classes=classes, depth=19, first_output=4, growth_rate=12, num_blocks=2, dropout=0.0,\\\n",
    "                    filter_size = 3, n_hidden1 = 40, n_hidden2 = 40)\n",
    "CU.SummaryNet(l_out, num_para=True, print_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sym_y = T.itensor3('target_contact_map')\n",
    "sym_mask = T.itensor3('mask')\n",
    "sym_weight = T.tensor3('contact_weight')\n",
    "\n",
    "\n",
    "grad_start_time = time.time()\n",
    "out_train = lasagne.layers.get_output(l_out, deterministic=False)\n",
    "out_eval = lasagne.layers.get_output(l_out, deterministic=True, batch_norm_use_averages=False)\n",
    "\n",
    "# reshape out for calculating loss\n",
    "pred_train = out_train[:, :, :, :].reshape((-1, classes))\n",
    "pred_valid = out_eval [:, :, :, :].reshape((-1, classes))\n",
    "\n",
    "params = lasagne.layers.get_all_params(l_out, regularizable=True)\n",
    "reg_term = sum(T.sum(p**2) for p in params)\n",
    "\n",
    "######### calculate cost function for train #######\n",
    "cost_train_cc = lasagne.objectives.categorical_crossentropy(T.clip(pred_train, TOL, 1-TOL), sym_y.flatten())\n",
    "cost_train_weight_cc = T.sum( cost_train_cc * sym_mask.flatten() * sym_weight.flatten() ) / \\\n",
    "                       T.sum( sym_weight.flatten() * sym_mask.flatten() )\n",
    "cost_train = cost_train_weight_cc + lambda_reg * reg_term\n",
    "\n",
    "######### calculate cost function for inference #######\n",
    "cost_valid_cc = lasagne.objectives.categorical_crossentropy(T.clip(pred_valid, TOL, 1-TOL), sym_y.flatten())\n",
    "cost_valid_weight_cc = T.sum(cost_valid_cc*sym_mask.flatten()*sym_weight.flatten()) / \\\n",
    "                       T.sum(sym_weight.flatten()*sym_mask.flatten())   # masked loss\n",
    "cost_inference = cost_valid_weight_cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cost function and computing grads...\n",
      "Using Adam Optimizer, with init step size 0.0010000000475 , done, time cosuming 63.6361210346 s\n"
     ]
    }
   ],
   "source": [
    "sh_lr = theano.shared(lasagne.utils.floatX(init_lr))\n",
    "all_params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "\n",
    "all_grads = [T.grad(cost_train, p, consider_constant= [sym_weight]) for p in all_params] \n",
    "print \"Creating cost function and computing grads...\"\n",
    "updates, norm_calc = lasagne.updates.total_norm_constraint(all_grads, max_norm=cut_norm, return_norm=True)\n",
    "\n",
    "if optimizer == 'adam':\n",
    "    print 'Using Adam Optimizer, with init step size', sh_lr.get_value(),\n",
    "    updates = lasagne.updates.adam(updates, all_params, learning_rate=sh_lr)\n",
    "elif optimizer == 'sgd':\n",
    "    print 'Using SGD Momentom Optimizer, with step size', sh_lr.get_value(),\n",
    "    updates = lasagne.updates.nesterov_momentum(updates, all_params, sh_lr, 0.9)\n",
    "print ', done, time cosuming', time.time() - grad_start_time, 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling train and eval...\n",
      "compile time 156.092435s\n"
     ]
    }
   ],
   "source": [
    "print 'compiling train and eval...'\n",
    "t_compile = time.time()\n",
    "train = theano.function([l_in_1.input_var, l_in_2.input_var, l_in_3.input_var, sym_y, sym_mask, sym_weight], \\\n",
    "                        [cost_train, cost_train_cc, cost_train_weight_cc, lambda_reg*reg_term, out_train], \\\n",
    "                        updates=updates, allow_input_downcast=True)\n",
    "\n",
    "eval_valid  = theano.function([l_in_1.input_var, l_in_2.input_var, l_in_3.input_var, sym_y, sym_mask, sym_weight], \\\n",
    "                        [cost_inference, cost_valid_cc, cost_valid_weight_cc, lambda_reg*reg_term, out_eval], \\\n",
    "                        allow_input_downcast=True)\n",
    "\n",
    "eval_train  = theano.function([l_in_1.input_var, l_in_2.input_var, l_in_3.input_var, sym_y, sym_mask, sym_weight], \\\n",
    "                        [cost_train, cost_train_cc, cost_train_weight_cc, lambda_reg*reg_term, out_train], \\\n",
    "                        allow_input_downcast=True)\n",
    "print \"compile time %fs\" %(time.time()-t_compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truncate data to MAX_LEN = 400\n"
     ]
    }
   ],
   "source": [
    "print 'Truncate data to MAX_LEN = 400'\n",
    "CU.TruncateTrainData(x_train, y_train, w_train, MAX_LEN = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 with lr = 0.001\n",
      "4004/4758 for train\t8.62694 2.00917 6.61777 0.015625\n",
      "103/400 for valid\t1.95672 1.95672 6.60978 0.0\n",
      "epoch 1 with lr = 0.001\n",
      "4004/4758 for train\t8.61298 2.00792 6.60506 0.015625\n",
      "103/400 for valid\t1.95598 1.95598 6.59676 0.0\n",
      "epoch 2 with lr = 0.001\n",
      "4004/4758 for train\t8.59857 2.00666 6.59191 0.015625\n",
      "103/400 for valid\t1.95528 1.95528 6.58341 0.0\n",
      "epoch 3 with lr = 0.001\n",
      "4004/4758 for train\t8.58398 2.00552 6.57846 0.015625\n",
      "103/400 for valid\t1.95469 1.95469 6.56982 0.0\n",
      "epoch 4 with lr = 0.001\n",
      "4004/4758 for train\t8.56937 2.00456 6.5648 0.015625\n",
      "103/400 for valid\t1.95422 1.95422 6.55606 0.0\n",
      "epoch 5 with lr = 0.0001\n",
      "4004/4758 for train\t8.55944 2.00389 6.55555 0.015625\n",
      "103/400 for valid\t1.95418 1.95418 6.55467 0.0\n",
      "epoch 6 with lr = 0.0001\n",
      "4004/4758 for train\t8.55798 2.00382 6.55416 0.015625\n",
      "103/400 for valid\t1.95414 1.95414 6.55326 0.0\n",
      "epoch 7 with lr = 0.0001\n",
      "4004/4758 for train\t8.55651 2.00376 6.55275 0.015625\n",
      "103/400 for valid\t1.9541 1.9541 6.55185 0.0\n",
      "epoch 8 with lr = 1e-05\n",
      "4004/4758 for train\t8.5555 2.0037 6.5518 0.015625\n",
      "103/400 for valid\t1.95409 1.95409 6.55171 0.0\n",
      "epoch 9 with lr = 1e-05\n",
      "4004/4758 for train\t8.55535 2.0037 6.55166 0.015625\n",
      "103/400 for valid\t1.95409 1.95409 6.55156 0.0\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0; num_epochs =10; log_file = open( output_dir + '/log.' + experiment_id, 'w', 0)\n",
    "\n",
    "loss_train_mean = []; acc_train_mean = []\n",
    "loss_valid_mean = []; acc_valid_mean = []\n",
    "loss_valid_mean2 = []; acc_valid_mean2 = []\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    start_time = time.time()\n",
    "    if shuffle:\n",
    "        combined = list(zip(x_train, w_train, y_train))\n",
    "        random.shuffle(combined)\n",
    "        x_train[:], w_train[:],  y_train[:] = zip(*combined)\n",
    "        \n",
    "    ################   Train  ##########################\n",
    "    loss_train_epoch = []; weight_train_epoch = [];  acc_train_epoch = []\n",
    "    print 'epoch', epoch, 'with lr =', np.round( sh_lr.get_value(), 6)\n",
    "    sys.stdout.flush\n",
    "    \n",
    "    for i in range(4000, 4004):\n",
    "        sys.stdout.write('\\r%d/%d for train'%(i+1, len(x_train) ))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        l, w, a = CU.RunFuncs(x_train[i], y_train[i], w_train[i], train)\n",
    "        loss_train_epoch.append(l); weight_train_epoch.append(w); acc_train_epoch.append(a)       \n",
    "    loss_train_mean.append( np.average(loss_train_epoch, axis=0, weights= weight_train_epoch) )\n",
    "    acc_train_mean.append( np.mean( acc_train_epoch, 0) )\n",
    "    print '\\t', ' '.join( [str(np.round(t,5)) for t in loss_train_mean[-1][[0,2,3]]] ), acc_train_mean[-1].mean(0)[4]\n",
    "    \n",
    "    ################   VALID  ##########################\n",
    "    loss_valid_epoch = []; weight_valid_epoch = [];  acc_valid_epoch = []\n",
    "    for i in range(100, 103):\n",
    "        sys.stdout.write('\\r%d/%d for valid'%(i+1, len(x_valid) ))\n",
    "        sys.stdout.flush()\n",
    "        l, w, a = CU.RunFuncs(x_valid[i], y_valid[i], w_valid[i], eval_valid)\n",
    "        loss_valid_epoch.append(l); weight_valid_epoch.append(w); acc_valid_epoch.append(a)       \n",
    "    loss_valid_mean.append( np.average(loss_valid_epoch, axis=0, weights= weight_valid_epoch) )\n",
    "    acc_valid_mean.append( np.mean( acc_valid_epoch, 0) )\n",
    "    print '\\t', ' '.join( [str(np.round(t,5)) for t in loss_valid_mean[-1][[0,2,3]]] ), acc_valid_mean[-1].mean(0)[4]\n",
    "\n",
    "    if reduce_lr and ( epoch+1 == (num_epochs-start_epoch) * 0.5 or \\\n",
    "                      epoch+1 == (num_epochs-start_epoch) * 0.8 ):\n",
    "        new_lr = sh_lr.get_value() * 0.1\n",
    "        sh_lr.set_value(lasagne.utils.floatX(new_lr))\n",
    "        \n",
    "    with open((metadata_path + \"-%d\" % (epoch) + \".pkl\"), 'w') as f:\n",
    "        cPickle.dump({'config_name': config_name, 'param_values': lasagne.layers.get_all_param_values(l_out)}, f, \\\n",
    "                 protocol=pickle.HIGHEST_PROTOCOL)\n",
    "       \n",
    "    print >> log_file, epoch+1, \n",
    "    print >> log_file, ' '.join( [str(np.round(t,5)) for t in loss_train_mean[-1][[0,2,3]]] ), \n",
    "    print >> log_file, ' '.join( [str(np.round(t,5)) for t in loss_valid_mean[-1][[0,2,3]]]), \n",
    "    print >> log_file, ' '.join(map(str, np.round(acc_train_mean[-1].mean(0),4))),\n",
    "    print >> log_file, ' '.join(map(str, np.round(acc_valid_mean[-1].mean(0),4))),\n",
    "    print >> log_file, time.time() - start_time\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'False'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = False\n",
    "str(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
